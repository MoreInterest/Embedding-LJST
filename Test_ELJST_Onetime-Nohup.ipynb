{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_embedding import BertEmbedding\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "from transformers import *\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "import imp, gzip\n",
    "import pickle, nltk\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as my_utils\n",
    "\n",
    "### Definitions\n",
    "\n",
    "def get_edges(i):\n",
    "    t = np.where(i>0)[0]\n",
    "    comb = combinations(t, 2)\n",
    "    embeds = {j:[] for j in t}\n",
    "\n",
    "    for p, q in comb:\n",
    "        if word_similarity[p][q]:\n",
    "            embeds[p] += [q]\n",
    "            embeds[q] += [p]\n",
    "    return embeds\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in tqdm(parse(path)):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "def process_df(df):\n",
    "    df['text'] = my_utils.preprocess(df['text'])\n",
    "    return df\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "def get_edges_transformers(text):\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    if embedding_name == 'bert':\n",
    "        results = bert_embedding(sentence)\n",
    "        embed_vecs = np.array([i[1][0] for i in results])\n",
    "    else:\n",
    "        embed_vecs = elmo.embed_sentence(sentence)[2]\n",
    "\n",
    "    l = np.array(list(set(sentence).intersection(words)))\n",
    "\n",
    "    pp = np.array([i[1] for i in nltk.pos_tag(l)])\n",
    "    pp[pp=='JJ'] = 1\n",
    "    pp[pp=='JJR'] = 1\n",
    "    pp[pp=='JJS'] = 1\n",
    "    pp[pp=='NN'] = 1\n",
    "    pp[pp=='NNS'] = 1\n",
    "    pp[pp=='NNP'] = 1\n",
    "    pp[pp=='NNPS'] = 1\n",
    "    pp[pp!='1'] = 0\n",
    "    pp = pp.astype(int)\n",
    "\n",
    "    l = l[pp==1]\n",
    "\n",
    "    word_embeddings = np.array([embed_vecs[sentence.index(i)] for i in l])\n",
    "\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    remove = np.where(word_similarity == 1)\n",
    "\n",
    "    for i, j in zip(remove[0], remove[1]):\n",
    "        word_similarity[i][j] = 0\n",
    "        word_similarity[j][i] = 0\n",
    "\n",
    "    word_similarity = word_similarity > cutoff\n",
    "    word_similarity = word_similarity.astype(int)\n",
    "    np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "    inds = np.where(word_similarity==1)\n",
    "\n",
    "    embeds = {words.index(j):[] for j in l}\n",
    "\n",
    "    for i, j in zip(inds[0], inds[1]):\n",
    "        embeds[words.index(l[i])] += [words.index(l[j])]\n",
    "\n",
    "    return embeds\n",
    "\n",
    "### Config\n",
    "\n",
    "dataset_names = [\"amazon_electronics\", \"amazon_home\", \"amazon_kindle\", \"amazon_movies\"]\n",
    "get_df_names = ['reviews_Electronics_5.json.gz', 'reviews_Home_and_Kitchen_5.json.gz', 'reviews_Kindle_Store_5.json.gz', 'reviews_Movies_and_TV_5.json.gz']\n",
    "\n",
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "cutoffs = [0.3, 0.6]\n",
    "\n",
    "n_cores = 40\n",
    "n_docs = 100000\n",
    "\n",
    "### Start\n",
    "glove_embedding_dim = 300\n",
    "glove_embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")\n",
    "fasttext_embedding_dim = 300\n",
    "fasttext_embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/wiki-news-300d-1M.vec\")\n",
    "\n",
    "for dataset_name, get_df_name in zip(dataset_names, get_df_names):\n",
    "    \n",
    "    print(\"********\", dataset_name, get_df_name)\n",
    "\n",
    "    dataset_ = getDF('datasets/' + get_df_name)\n",
    "    dataset = dataset_.sample(n_docs*3)\n",
    "    dataset = dataset.drop(columns=['reviewerID', 'asin', 'reviewerName', 'helpful', 'summary', 'unixReviewTime', 'reviewTime'])\n",
    "    dataset = dataset.rename(columns={'reviewText': 'text', 'overall': 'sentiment'})\n",
    "\n",
    "    n = int(dataset.shape[0]/n_cores)\n",
    "    list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    processed_list_df = pool.map(process_df, list_df)\n",
    "    pool.close()\n",
    "\n",
    "    dataset = pd.concat(processed_list_df)\n",
    "    dataset = dataset[dataset.text.apply(lambda x: len(x.split(\" \"))>5 and len(x.split(\" \"))<200)].sample(n_docs).reset_index().drop(columns='index')\n",
    "    dataset.to_pickle(\"resources/\"+ dataset_name + \"_\" + str(n_docs) + \"_dataset\")\n",
    "    dataset_. = None\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                                 stop_words=\"english\", max_features=max_features,\n",
    "                                 max_df=max_df, min_df=min_df)\n",
    "\n",
    "    wordOccurenceMatrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "\n",
    "    words = vectorizer.get_feature_names()\n",
    "    bertvocab = words + ['[CLS]', '[UNK]']\n",
    "    pd.DataFrame(bertvocab).to_csv(\"bertvocab.text\", header=None, index=None)\n",
    "\n",
    "    # Embeddings\n",
    "\n",
    "    ### Glove\n",
    "    print(\"Glove\")\n",
    "    glove_embedding_dim = 300\n",
    "    glove_embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")\n",
    "\n",
    "    glove_word_embeddings = []\n",
    "\n",
    "    for word in tqdm(words):\n",
    "        emb = glove_embeddings_index.get(word, np.array([0]*glove_embedding_dim))\n",
    "        glove_word_embeddings.append(emb.tolist())\n",
    "\n",
    "    glove_word_embeddings = np.array(glove_word_embeddings)\n",
    "\n",
    "    g = ['glove', glove_word_embeddings]\n",
    "\n",
    "    ### Fasttext\n",
    "    print(\"Fasttext\")\n",
    "    fasttext_embedding_dim = 300\n",
    "    fasttext_embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/wiki-news-300d-1M.vec\")\n",
    "\n",
    "    fasttext_word_embeddings = []\n",
    "\n",
    "    for word in tqdm(words):\n",
    "        emb = np.array([0]*glove_embedding_dim)\n",
    "        try:\n",
    "            emb = fasttext_embeddings_index[word]\n",
    "        except:\n",
    "            pass\n",
    "        fasttext_word_embeddings.append(emb.tolist())\n",
    "\n",
    "    fasttext_word_embeddings = np.array(fasttext_word_embeddings)\n",
    "\n",
    "    f = ['fasttext', fasttext_word_embeddings]\n",
    "\n",
    "    #### Grid\n",
    "    print(\"Grid\")\n",
    "    for embedding_name, word_embeddings in [g, f]:\n",
    "        for cutoff in cutoffs:\n",
    "            print(embedding_name, cutoff)\n",
    "            word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "            remove = np.where(word_similarity == 1)\n",
    "\n",
    "            for i, j in zip(remove[0], remove[1]):\n",
    "                word_similarity[i][j] = 0\n",
    "                word_similarity[j][i] = 0\n",
    "\n",
    "            word_similarity = word_similarity > cutoff\n",
    "            word_similarity = word_similarity.astype(int)\n",
    "            np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "            wordOccuranceMatrixBinary = wordOccurenceMatrix.copy()\n",
    "            wordOccuranceMatrixBinary[wordOccuranceMatrixBinary > 1] = 1\n",
    "\n",
    "            pool = multiprocessing.Pool(n_cores)\n",
    "            similar_words = pool.map(get_edges, wordOccuranceMatrixBinary)\n",
    "            pool.close()\n",
    "            pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "            pickle.dump(similar_words, pickle_out)\n",
    "            pickle_out.close()\n",
    "\n",
    "    ## Bert Embedding & Attention\n",
    "    print(\"Bert\")\n",
    "    embedding_name = 'bert'\n",
    "\n",
    "    cutoff = 0.95\n",
    "\n",
    "    pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "    model = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "    tokenizer = BertTokenizer(vocab_file='bertvocab.txt', never_split=True, do_basic_tokenize=False)\n",
    "\n",
    "    tokenized_text = [tokenizer.tokenize(i) for i in dataset.text]\n",
    "\n",
    "    temp = []\n",
    "    for i in tokenized_text:\n",
    "        t = [j for j in i if j in words]\n",
    "        temp.append(t)\n",
    "\n",
    "    tokenized_text = temp\n",
    "\n",
    "    indexed_tokens = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n",
    "\n",
    "    input_ids = keras.preprocessing.sequence.pad_sequences(indexed_tokens, padding='post', dtype='long', maxlen=max([len(i) for i in indexed_tokens]))\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "\n",
    "    input_ids = torch.split(input_ids, 500, dim=0)\n",
    "\n",
    "    pad_length = [len(i) for i in indexed_tokens]\n",
    "\n",
    "    print(\"Start Bert...\")\n",
    "    idx = 0\n",
    "    similar_words_bert = []\n",
    "    similar_words_bert_attention = []\n",
    "\n",
    "    for batch in tqdm(input_ids):\n",
    "\n",
    "        all_embeddings, _, _, all_attentions = model(batch)\n",
    "        idx_copy = deepcopy(idx)\n",
    "\n",
    "        for one_embedding in all_embeddings.detach().numpy():\n",
    "            word_embeddings = one_embedding[:pad_length[idx]]\n",
    "            word_similarity = cosine_similarity(word_embeddings)\n",
    "            remove = np.where(word_similarity == 1.000) # to remove self words coupling\n",
    "\n",
    "            for i, j in zip(remove[0], remove[1]):\n",
    "                word_similarity[i][j] = 0\n",
    "                word_similarity[j][i] = 0\n",
    "\n",
    "            word_similarity = word_similarity > cutoff\n",
    "            word_similarity = word_similarity.astype(int)\n",
    "            np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "            inds = np.where(word_similarity==1)\n",
    "            embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "            for i, j in zip(inds[0], inds[1]):\n",
    "                embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "            similar_words_bert.append(embeds)\n",
    "\n",
    "        idx = deepcopy(idx_copy)\n",
    "\n",
    "        for one_attentions in all_attentions[0].detach().numpy():\n",
    "\n",
    "            one_side_edges = np.argmax(one_attentions[9], axis=1) #taking 9 layer of attention\n",
    "            embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "            for j, i in enumerate(one_side_edges[:pad_length[idx]]):\n",
    "                if i < pad_length[idx]:\n",
    "                    embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "            similar_words_bert_attention.append(embeds)\n",
    "            idx += 1\n",
    "\n",
    "    print(\"Done\")\n",
    "    pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert' + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "    pickle.dump(similar_words_bert, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert_attention'+ \".pickle\",\"wb\")\n",
    "    pickle.dump(similar_words_bert_attention, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### POS\n",
    "#         pp = np.array([i[1] for i in nltk.pos_tag(words)])\n",
    "#         pp[pp=='JJ'] = 1\n",
    "#         pp[pp=='JJR'] = 1\n",
    "#         pp[pp=='JJS'] = 1\n",
    "#         pp[pp=='NN'] = 1\n",
    "#         pp[pp=='NNS'] = 1\n",
    "#         pp[pp=='NNP'] = 1\n",
    "#         pp[pp=='NNPS'] = 1\n",
    "#         pp[pp!='1'] = 0\n",
    "#         pp = pp.astype(int)\n",
    "\n",
    "#         wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[0].sum()\n",
    "\n",
    "# np.sum(wordOccuranceMatrixBinary)\n",
    "\n",
    "# Counter(np.array([i[1] for i in nltk.pos_tag(words)]))\n",
    "\n",
    "# pp.sum()\n",
    "\n",
    "# np.where(pp!=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for embedding_name in ['bert', 'elmo']:\n",
    "#     for cutoff in cutoffs:\n",
    "#         print(embedding_name, cutoff)\n",
    "#         pool = multiprocessing.Pool(n_cores)\n",
    "#         similar_words = pool.map(get_edges_transformers, dataset.text.tolist())\n",
    "#         pool.close()\n",
    "#         pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) + \"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "#         pickle.dump(similar_words, pickle_out)\n",
    "#         pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pd = pd.apply(lambda x: convert_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_df(df):\n",
    "#     df['text'] = preprocess(df['reviewText'])\n",
    "    \n",
    "# #     pool = multiprocessing.Pool(n_cores)\n",
    "# #     df['cleaned'] = pool.map(process_l, df['text'].tolist())\n",
    "# #     pool.close()\n",
    "    \n",
    "# #     df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
