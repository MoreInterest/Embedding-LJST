{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/datadf_amazon_patio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1JZFGZEZVWQPY</td>\n",
       "      <td>B00002N674</td>\n",
       "      <td>Carter H \"1amazonreviewer@gmail . com\"</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>Good USA company that stands behind their prod...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great Hoses</td>\n",
       "      <td>1308614400</td>\n",
       "      <td>06 21, 2011</td>\n",
       "      <td>[good, usa, company, stand, behind, product, w...</td>\n",
       "      <td>good usa company stand behind product warranty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A32JCI4AK2JTTG</td>\n",
       "      <td>B00002N674</td>\n",
       "      <td>Darryl Bennett \"Fuzzy342\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a high quality 8 ply hose. I have had ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>[high, quality, ply, hose, good, luck, gilmour...</td>\n",
       "      <td>high quality ply hose good luck gilmour hose p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                            reviewerName helpful  \\\n",
       "0  A1JZFGZEZVWQPY  B00002N674  Carter H \"1amazonreviewer@gmail . com\"  [4, 4]   \n",
       "1  A32JCI4AK2JTTG  B00002N674               Darryl Bennett \"Fuzzy342\"  [0, 0]   \n",
       "\n",
       "                                          reviewText  sentiment  \\\n",
       "0  Good USA company that stands behind their prod...        4.0   \n",
       "1  This is a high quality 8 ply hose. I have had ...        5.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                        Great Hoses      1308614400   \n",
       "1  Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...      1402272000   \n",
       "\n",
       "    reviewTime                                            cleaned  \\\n",
       "0  06 21, 2011  [good, usa, company, stand, behind, product, w...   \n",
       "1   06 9, 2014  [high, quality, ply, hose, good, luck, gilmour...   \n",
       "\n",
       "                                                text  \n",
       "0  good usa company stand behind product warranty...  \n",
       "1  high quality ply hose good luck gilmour hose p...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix, tfidf_matrix, vocabulary, words = my_utils.processReviews(dataset['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 22s, sys: 3.38 s, total: 4min 26s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = {}\n",
    "for i in words:\n",
    "    try:\n",
    "        words_embeddings[i] = embeddings_index[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_embeddings = words_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 604 ms, sys: 27.9 ms, total: 632 ms\n",
      "Wall time: 631 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edge_embeds_multi = []\n",
    "for i, j in combinations(words_with_embeddings, 2):\n",
    "    edge_embeds_multi.append((words_embeddings[i], words_embeddings[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1983036"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_embeds_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.72 s, sys: 2.15 s, total: 7.87 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "embeddings_cosines = pool.map(my_utils.get_cosine_multi, edge_embeds_multi)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1983036"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 384 ms, total: 2.27 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edge_embeddings = {}\n",
    "for idx, (i, j) in enumerate(combinations(words_with_embeddings, 2)):\n",
    "    edge_embeddings[(i, j)] = embeddings_cosines[idx]\n",
    "    edge_embeddings[(j, i)] = embeddings_cosines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_per_doc(doc):\n",
    "    edges, edges_all = [], []\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j and i in words_with_embeddings and j in words_with_embeddings:\n",
    "                sim = edge_embeddings[(i, j)]\n",
    "                if sim > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "                    edges.append((vocabulary[i], vocabulary[j]))\n",
    "                    edges_all.append((i, j, sim))\n",
    "    return (edges, edges_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "docs_edges_multi = pool.map(get_edges_per_doc, dataset['cleaned'].values)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_edges = [i[0] for i in docs_edges_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_edges_all = [i[1] for i in docs_edges_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['text'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_edges_all[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median([len(i) for i in docs_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean([len(i) for i in docs_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array([len(i) for i in docs_edges]) == 0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter([len(i) for i in docs_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"resources/edges_amazon_patio_fasttext_nontrained.pickle\",\"wb\")\n",
    "pickle.dump(docs_edges, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# edges_threshold = 0.73\n",
    "# docs_edges = []\n",
    "# docs_edges_all = []\n",
    "# for idx, doc in enumerate(dataset[10].values):\n",
    "#     my_utils.print_if_mod(idx, 500)\n",
    "#     edges, edges_all = [], []\n",
    "#     for i in doc:\n",
    "#         for j in doc:\n",
    "#             if i != j:\n",
    "#                 sim = edge_embeddings[(i, j)]\n",
    "#                 if sim > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "#                     edges.append((vocabulary[i], vocabulary[j]))\n",
    "#                     edges_all.append((i, j, sim))\n",
    "#     docs_edges.append(edges)\n",
    "#     docs_edges_all.append(edges_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadGloveModel(gloveFile):\n",
    "#     print(\"Loading Glove Model\")\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#         model[word] = embedding\n",
    "#     print(\"Done.\",len(model),\" words loaded!\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = loadGloveModel(\"nongit_resources/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/electronics.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# edges_threshold = 0.8\n",
    "# docs_edges, ignored, taken, count = [], [], [], 0\n",
    "# for idx, doc in enumerate(dataset[8].values):\n",
    "#     edges = []\n",
    "#     print(idx)\n",
    "#     for i in doc:\n",
    "#         for j in doc:\n",
    "#             if i != j:\n",
    "#                 try:\n",
    "#                     a = embeddings_index[i]\n",
    "#                     b = embeddings_index[j]\n",
    "#                     if get_cosine(a, b) > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "#                         edges.append((vocabulary[i], vocabulary[j]))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         embeddings_index[i]\n",
    "#                         taken.append(i)\n",
    "#                     except:\n",
    "#                         ignored.append(i)\n",
    "#                     try:\n",
    "#                         embeddings_index[j]\n",
    "#                     except:\n",
    "#                         ignored.append(j)\n",
    "#                         taken.append(j)\n",
    "#                     pass\n",
    "#     docs_edges.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/docs_edges_\" + dataset_name + \"_5k_fasttext_trained.pickle\",\"wb\")\n",
    "# pickle.dump(docs_edges, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence wise\n",
    "# dataset = dataset[['asin', 'helpful', 'overall', 'reviewText']]\n",
    "# dataset['n_words'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x)))\n",
    "# dataset['sentences'] = dataset['reviewText'].apply(lambda x: [i.strip() for i in x.split(\".\")])\n",
    "# dataset['sentence_word_density'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x))/ len(x.split(\".\")))\n",
    "# dataset.to_csv(\"reviews_Musical_Instruments_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# edge_dict, ignored, taken, count = {}, [], [], 0\n",
    "# for idxi, i in enumerate(vocabulary.keys()):\n",
    "#     print(idxi)\n",
    "#     for idxj, j in enumerate(vocabulary.keys()):\n",
    "#         if i != j:\n",
    "#             try:\n",
    "#                 a = embeddings_index[i]\n",
    "#                 b = embeddings_index[j]\n",
    "#                 if get_cosine(a, b) > edges_threshold:\n",
    "#                     try:\n",
    "#                         edge_dict[vocabulary[i]] += [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] += [vocabulary[i]]\n",
    "#                     except:\n",
    "#                         edge_dict[vocabulary[i]] = [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] = [vocabulary[i]]\n",
    "#             except:\n",
    "#                 try:\n",
    "#                     embeddings_index[i]\n",
    "#                     taken.append(i)\n",
    "#                 except:\n",
    "#                     ignored.append(i)\n",
    "#                 try:\n",
    "#                     embeddings_index[j]\n",
    "#                 except:\n",
    "#                     ignored.append(j)\n",
    "#                     taken.append(j)\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {}\n",
    "\n",
    "# for idx, i in enumerate(dataset[8].values):\n",
    "#     print(idx)\n",
    "#     for j in i:\n",
    "#         try:\n",
    "#             df[j] += [idx]\n",
    "#         except:\n",
    "#             df[j] = [idx]\n",
    "\n",
    "# for i in df.keys():\n",
    "#     df[i] = len(list(set(df[i])))\n",
    "\n",
    "# df_vector = []\n",
    "# for i in dataset[8].values:\n",
    "#     d = [0]*len(vocabulary.keys())\n",
    "#     for j in i:\n",
    "#         if j in vocabulary.keys():\n",
    "#             d[vocabulary[j]] = df[j]\n",
    "#     df_vector.append(d)\n",
    "\n",
    "# csr = sparse.csr_matrix(np.array(df_vector))\n",
    "# scipy.sparse.save_npz('resources/df_stackoverflow_5kanswers.npz', csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(scipy.sparse.load_npz('resources/df_stackoverflow_5kanswers.npz').todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = parse(\"nongit_resources/reviews_Electronics_5.json.gz\")\n",
    "# dataset = pd.DataFrame(list(dataset))\n",
    "# dataset = dataset.head(N_docs)\n",
    "# dataset.to_pickle(\"resources/reviews_Electronics_5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
