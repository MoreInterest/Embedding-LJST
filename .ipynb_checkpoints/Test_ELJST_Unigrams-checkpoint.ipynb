{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william18026/miniconda3/envs/python3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial, sparse\n",
    "from scipy.stats import chi2\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.externals import joblib \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import gzip\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import LDA_ELJST as lda\n",
    "import ELJST_script_unigram as lda\n",
    "#import LJST_script_BTM as lda\n",
    "#import ELJST_script_BTM as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['cleaned'] = my_utils.preprocess(df['reviewText'])\n",
    "    df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = getDF('datasets/reviews_Musical_Instruments_5.json.gz')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(dataset.shape[0]/n_cores)\n",
    "list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "processed_list_df = pool.map(process_df, list_df)\n",
    "pool.close()\n",
    "\n",
    "dataset = pd.concat(processed_list_df)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dataset.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_dim = 300\n",
    "embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "n_sentiment = 5\n",
    "alpha = 0.1/n_topics * np.ones(n_topics)\n",
    "beta = .01\n",
    "gamma = 10\n",
    "gamma = [gamma/(n_topics*n_sentiment)]*n_sentiment\n",
    "\n",
    "maxiter = 10\n",
    "lambda_param = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.SentimentLDAGibbsSampler(n_topics, alpha, beta, gamma, numSentiments=n_sentiment, minlabel = 0, \n",
    "                                       maxlabel = 5, SentimentRange = 5, max_df = .5, min_df = 5, \n",
    "                                       lambda_param = lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler._initialize_(reviews = dataset.text.tolist(), labels = dataset.overall.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_embeddings = []\n",
    "\n",
    "for word in tqdm(sampler.words):\n",
    "    emb = embeddings_index.get(word, np.array([0]*embedding_dim))\n",
    "    word_embeddings.append(emb)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = .5\n",
    "word_similarity = cosine_similarity(word_embeddings)\n",
    "word_similarity = word_similarity > cutoff\n",
    "word_similarity = word_similarity.astype(int)\n",
    "np.fill_diagonal(word_similarity, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOccuranceMatrixBinary = sampler.wordOccuranceMatrix.copy()\n",
    "wordOccuranceMatrixBinary[wordOccuranceMatrixBinary > 1] = 1\n",
    "wordOccuranceMatrixBinary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(wordOccuranceMatrixBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(np.array([i[1] for i in nltk.pos_tag(sampler.words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.array([i[1] for i in nltk.pos_tag(sampler.words)])\n",
    "pp[pp=='JJ'] = 1\n",
    "pp[pp=='JJR'] = 1\n",
    "pp[pp=='JJS'] = 1\n",
    "pp[pp=='NN'] = 1\n",
    "pp[pp=='NNS'] = 1\n",
    "pp[pp=='NNP'] = 1\n",
    "pp[pp=='NNPS'] = 1\n",
    "pp[pp!='1'] = 0\n",
    "pp = pp.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOccuranceMatrixBinary[np.where(pp!=1)[0], :] = 0\n",
    "wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n",
    "np.sum(wordOccuranceMatrixBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(i):\n",
    "    t = np.where(i==0)[0]\n",
    "    k = word_similarity.copy()\n",
    "\n",
    "    k[t, :] = 0\n",
    "    k[:, t] = 0\n",
    "\n",
    "    z, x = np.where(k==1)\n",
    "    embeds = {}\n",
    "    for p, q in zip(z, x):\n",
    "        try:\n",
    "            embeds[p] += [q]\n",
    "        except:\n",
    "            embeds[p] = [q]\n",
    "        try:\n",
    "            embeds[q] += [p]\n",
    "        except:\n",
    "            embeds[q] = [p]\n",
    "    for i in embeds.keys():\n",
    "        embeds[i] = list(set(embeds[i]))\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# similar_words = pool.map(get_edges, wordOccuranceMatrixBinary)\n",
    "# pool.close()\n",
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.5_POS.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = pickle.load(open(\"resources/amazon_muiscal_glove_0.5_POS.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler.run(reviews=dataset.text.tolist(), labels=dataset.overall.tolist(), \n",
    "            similar_words=similar_words, mrf=True, maxIters=maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([len(i) for i in sampler.docs_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.loglikelihood_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),\n",
    "                 sampler.dt_distribution.argmax(axis=1), metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix, sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, list(sampler.getTopKWords(5).values()), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_utils.get_hscore_multi(sampler.dt_distribution, sampler.wordOccuranceMatrix, n_topics, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sampler, \"resources/sampler_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
