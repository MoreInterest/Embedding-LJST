{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william18026/.conda/envs/python3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial, sparse\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.externals import joblib \n",
    "from nltk.tokenize import word_tokenize\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import imp\n",
    "import gzip\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import LDA_ELJST as lda\n",
    "import ELJST_script_unigram as lda\n",
    "#import LJST_script_BTM as lda\n",
    "#import ELJST_script_BTM as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial, sparse\n",
    "from scipy.stats import chi2\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import copy\n",
    "import nltk\n",
    "import scipy\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(i):\n",
    "    t = np.where(i>0)[0]\n",
    "    comb = combinations(t, 2)    \n",
    "    embeds = {j:[] for j in t}\n",
    "\n",
    "    for p, q in comb:\n",
    "        if word_similarity[p][q]:\n",
    "            embeds[p] += [q]\n",
    "            embeds[q] += [p]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in tqdm(parse(path)):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['text'] = preprocess(df['reviewText'])\n",
    "    \n",
    "#     pool = multiprocessing.Pool(n_cores)\n",
    "#     df['cleaned'] = pool.map(process_l, df['text'].tolist())\n",
    "#     pool.close()\n",
    "    \n",
    "#     df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ = getDF('datasets/reviews_Electronics_5.json.gz')\n",
    "# dataset_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_.head(10000).to_pickle(\"resources/electronics_10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset_.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"resources/electronics_10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.str.replace('\\d+', ' ')\n",
    "    pd = pd.apply(lambda x: [w for w in w_tokenizer.tokenize(x)])\n",
    "#     pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w, 'v') for w in x])\n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    pd = pd.apply(lambda x: [item for item in x if len(item)>2])\n",
    "    pd = pd.apply(lambda x: \" \".join(x))\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 229 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = process_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(dataset.shape[0]/n_cores)\n",
    "list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "processed_list_df = pool.map(process_df, list_df)\n",
    "pool.close()\n",
    "\n",
    "dataset = pd.concat(processed_list_df)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5.0: 6061, 1.0: 572, 3.0: 822, 2.0: 450, 4.0: 2095})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dataset.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1370131200</td>\n",
       "      <td>06 2, 2013</td>\n",
       "      <td>get gps husband otr road trucker impress ship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1290643200</td>\n",
       "      <td>11 25, 2010</td>\n",
       "      <td>professional otr truck driver buy tnd truck st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1283990400</td>\n",
       "      <td>09 9, 2010</td>\n",
       "      <td>well say unit truck four day prior garmin non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>11 24, 2010</td>\n",
       "      <td>write long review even think unit deserve one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>09 29, 2011</td>\n",
       "      <td>mine year get try route non truck route tell t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin              reviewerName   helpful  \\\n",
       "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
       "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
       "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
       "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
       "4  A24EV6RXELQZ63  0528881469               Wayne Smith    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
       "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
       "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
       "3  Not going to write a long review, even thought...      2.0   \n",
       "4  I've had mine for a year and here's what we go...      1.0   \n",
       "\n",
       "                                  summary  unixReviewTime   reviewTime  \\\n",
       "0                         Gotta have GPS!      1370131200   06 2, 2013   \n",
       "1                       Very Disappointed      1290643200  11 25, 2010   \n",
       "2                          1st impression      1283990400   09 9, 2010   \n",
       "3                 Great grafics, POOR GPS      1290556800  11 24, 2010   \n",
       "4  Major issues, only excuses for support      1317254400  09 29, 2011   \n",
       "\n",
       "                                                text  \n",
       "0  get gps husband otr road trucker impress ship ...  \n",
       "1  professional otr truck driver buy tnd truck st...  \n",
       "2  well say unit truck four day prior garmin non ...  \n",
       "3  write long review even think unit deserve one ...  \n",
       "4  mine year get try route non truck route tell t...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embedding_dim = 300\n",
    "# embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "n_sentiment = 5\n",
    "alpha = 0.1/n_topics * np.ones(n_topics)\n",
    "beta = .01\n",
    "gamma = 10\n",
    "gamma = [gamma/(n_topics*n_sentiment)]*n_sentiment\n",
    "\n",
    "maxiter = 20\n",
    "lambda_param = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4, 0.4, 0.4, 0.4, 0.4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ELJST_script_unigram' from '/home/william18026/Embedding-LJST/ELJST_script_unigram.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.SentimentLDAGibbsSampler(n_topics, alpha, beta, gamma, numSentiments=n_sentiment, minlabel = 0, \n",
    "                                       maxlabel = 5, SentimentRange = 5, max_df = .5, min_df = 5, \n",
    "                                       lambda_param = lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4852 (10000, 4852)\n",
      "CPU times: user 25.4 s, sys: 572 ms, total: 26 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampler._initialize_(reviews = dataset.text.tolist(), labels = dataset.overall.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.wordOccuranceMatrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# word_embeddings = []\n",
    "\n",
    "# for word in tqdm(sampler.words):\n",
    "#     emb = embeddings_index.get(word, np.array([0]*embedding_dim))\n",
    "#     word_embeddings.append(emb.tolist())\n",
    "\n",
    "# word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = .35\n",
    "# word_similarity = cosine_similarity(word_embeddings)\n",
    "# word_similarity = word_similarity > cutoff\n",
    "# word_similarity = word_similarity.astype(int)\n",
    "# np.fill_diagonal(word_similarity, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary = sampler.wordOccuranceMatrix.copy()\n",
    "# wordOccuranceMatrixBinary[wordOccuranceMatrixBinary > 1] = 1\n",
    "# wordOccuranceMatrixBinary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(wordOccuranceMatrixBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counter(np.array([i[1] for i in nltk.pos_tag(sampler.words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp = np.array([i[1] for i in nltk.pos_tag(sampler.words)])\n",
    "# pp[pp=='JJ'] = 1\n",
    "# pp[pp=='JJR'] = 1\n",
    "# pp[pp=='JJS'] = 1\n",
    "# pp[pp=='NN'] = 1\n",
    "# pp[pp=='NNS'] = 1\n",
    "# pp[pp=='NNP'] = 1\n",
    "# pp[pp=='NNPS'] = 1\n",
    "# pp[pp!='1'] = 0\n",
    "# pp = pp.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(pp!=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n",
    "# np.sum(wordOccuranceMatrixBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# similar_words = pool.map(get_edges, wordOccuranceMatrixBinary)\n",
    "# pool.close()\n",
    "# pickle_out = open(\"resources/amazon_electronics_glove_0.35_POS_10k.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = pickle.load(open(\"resources/amazon_electronics_glove_0.35_POS_10k.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:00<12:35, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:27<00:00, 30.52it/s]\n",
      "  0%|          | 2/10000 [00:00<12:36, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:27<00:00, 30.54it/s]\n",
      "  0%|          | 2/10000 [00:00<12:31, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 3 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5380/10000 [02:55<01:17, 59.50it/s]"
     ]
    }
   ],
   "source": [
    "sampler.run(reviews=dataset.text.tolist(), labels=dataset.overall.tolist(), \n",
    "            similar_words=similar_words, mrf=True, maxIters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([len(i) for i in sampler.docs_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.loglikelihood_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),\n",
    "                 sampler.dt_distribution.argmax(axis=1), metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix, sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, list(sampler.getTopKWords(5).values()), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_utils.get_hscore_multi(sampler.dt_distribution, sampler.wordOccuranceMatrix, n_topics, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
