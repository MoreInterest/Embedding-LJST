{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils as my_utils\n",
    "\n",
    "### Required Methods\n",
    "\n",
    "dataset = pd.read_pickle(\"datasets/datadf_amazon_musical\")\n",
    "\n",
    "dataset.head(2)\n",
    "\n",
    "count_matrix, _, vocabulary, words = my_utils.processReviews(dataset['text'].values)\n",
    "\n",
    "words_embeddings = {}\n",
    "for i in words:\n",
    "    try:\n",
    "        words_embeddings[i] = embeddings_index[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "embeddings_index = None\n",
    "\n",
    "words_with_embeddings = words_embeddings.keys()\n",
    "\n",
    "edge_embeds_multi = []\n",
    "for i, j in combinations(words_with_embeddings, 2):\n",
    "    edge_embeds_multi.append((words_embeddings[i], words_embeddings[j]))\n",
    "\n",
    "len(edge_embeds_multi)\n",
    "\n",
    "n_cores = 30\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "embeddings_cosines = pool.map(my_utils.get_cosine_multi, edge_embeds_multi)\n",
    "pool.close()\n",
    "\n",
    "edge_embeddings = {}\n",
    "for idx, (i, j) in enumerate(combinations(words_with_embeddings, 2)):\n",
    "    edge_embeddings[(i, j)] = embeddings_cosines[idx]\n",
    "    edge_embeddings[(j, i)] = embeddings_cosines[idx]\n",
    "\n",
    "def get_edges_per_doc(doc):\n",
    "    edges, edges_all = [], []\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j and i in words_with_embeddings and j in words_with_embeddings:\n",
    "                sim = edge_embeddings[(i, j)]\n",
    "                if sim > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "                    edges.append((vocabulary[i], vocabulary[j]))\n",
    "                    edges_all.append((i, j, sim))\n",
    "    return (edges, edges_all)\n",
    "\n",
    "edges_threshold = 0.00\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "docs_edges_multi = pool.map(get_edges_per_doc, dataset['cleaned'].values)\n",
    "pool.close()\n",
    "\n",
    "docs_edges = [i[0] for i in docs_edges_multi]\n",
    "\n",
    "docs_edges_all = [i[1] for i in docs_edges_multi]\n",
    "\n",
    "pickle_out = open(\"resources/amazon_musical_glove_nontrained_0.00.pickle\",\"wb\")\n",
    "pickle.dump(docs_edges, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "for edges_threshold in [0.60, 0.50, 0.40, 0.30, 0.20, 0.10, 0.00]:\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    docs_edges_multi = pool.map(get_edges_per_doc, dataset['cleaned'].values)\n",
    "    pool.close()\n",
    "\n",
    "    docs_edges = [i[0] for i in docs_edges_multi]\n",
    "\n",
    "    docs_edges_all = [i[1] for i in docs_edges_multi]\n",
    "\n",
    "    pickle_out = open(\"resources/amazon_musical_fasttext_\" + str(edges_threshold) + \".pickle\",\"wb\")\n",
    "    pickle.dump(docs_edges, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# edges_threshold = 0.73\n",
    "# docs_edges = []\n",
    "# docs_edges_all = []\n",
    "# for idx, doc in enumerate(dataset[10].values):\n",
    "#     my_utils.print_if_mod(idx, 500)\n",
    "#     edges, edges_all = [], []\n",
    "#     for i in doc:\n",
    "#         for j in doc:\n",
    "#             if i != j:\n",
    "#                 sim = edge_embeddings[(i, j)]\n",
    "#                 if sim > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "#                     edges.append((vocabulary[i], vocabulary[j]))\n",
    "#                     edges_all.append((i, j, sim))\n",
    "#     docs_edges.append(edges)\n",
    "#     docs_edges_all.append(edges_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadGloveModel(gloveFile):\n",
    "#     print(\"Loading Glove Model\")\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#         model[word] = embedding\n",
    "#     print(\"Done.\",len(model),\" words loaded!\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = loadGloveModel(\"nongit_resources/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/electronics.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# edges_threshold = 0.8\n",
    "# docs_edges, ignored, taken, count = [], [], [], 0\n",
    "# for idx, doc in enumerate(dataset[8].values):\n",
    "#     edges = []\n",
    "#     print(idx)\n",
    "#     for i in doc:\n",
    "#         for j in doc:\n",
    "#             if i != j:\n",
    "#                 try:\n",
    "#                     a = embeddings_index[i]\n",
    "#                     b = embeddings_index[j]\n",
    "#                     if get_cosine(a, b) > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "#                         edges.append((vocabulary[i], vocabulary[j]))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         embeddings_index[i]\n",
    "#                         taken.append(i)\n",
    "#                     except:\n",
    "#                         ignored.append(i)\n",
    "#                     try:\n",
    "#                         embeddings_index[j]\n",
    "#                     except:\n",
    "#                         ignored.append(j)\n",
    "#                         taken.append(j)\n",
    "#                     pass\n",
    "#     docs_edges.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/docs_edges_\" + dataset_name + \"_5k_fasttext_trained.pickle\",\"wb\")\n",
    "# pickle.dump(docs_edges, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence wise\n",
    "# dataset = dataset[['asin', 'helpful', 'overall', 'reviewText']]\n",
    "# dataset['n_words'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x)))\n",
    "# dataset['sentences'] = dataset['reviewText'].apply(lambda x: [i.strip() for i in x.split(\".\")])\n",
    "# dataset['sentence_word_density'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x))/ len(x.split(\".\")))\n",
    "# dataset.to_csv(\"reviews_Musical_Instruments_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# edge_dict, ignored, taken, count = {}, [], [], 0\n",
    "# for idxi, i in enumerate(vocabulary.keys()):\n",
    "#     print(idxi)\n",
    "#     for idxj, j in enumerate(vocabulary.keys()):\n",
    "#         if i != j:\n",
    "#             try:\n",
    "#                 a = embeddings_index[i]\n",
    "#                 b = embeddings_index[j]\n",
    "#                 if get_cosine(a, b) > edges_threshold:\n",
    "#                     try:\n",
    "#                         edge_dict[vocabulary[i]] += [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] += [vocabulary[i]]\n",
    "#                     except:\n",
    "#                         edge_dict[vocabulary[i]] = [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] = [vocabulary[i]]\n",
    "#             except:\n",
    "#                 try:\n",
    "#                     embeddings_index[i]\n",
    "#                     taken.append(i)\n",
    "#                 except:\n",
    "#                     ignored.append(i)\n",
    "#                 try:\n",
    "#                     embeddings_index[j]\n",
    "#                 except:\n",
    "#                     ignored.append(j)\n",
    "#                     taken.append(j)\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {}\n",
    "\n",
    "# for idx, i in enumerate(dataset[8].values):\n",
    "#     print(idx)\n",
    "#     for j in i:\n",
    "#         try:\n",
    "#             df[j] += [idx]\n",
    "#         except:\n",
    "#             df[j] = [idx]\n",
    "\n",
    "# for i in df.keys():\n",
    "#     df[i] = len(list(set(df[i])))\n",
    "\n",
    "# df_vector = []\n",
    "# for i in dataset[8].values:\n",
    "#     d = [0]*len(vocabulary.keys())\n",
    "#     for j in i:\n",
    "#         if j in vocabulary.keys():\n",
    "#             d[vocabulary[j]] = df[j]\n",
    "#     df_vector.append(d)\n",
    "\n",
    "# csr = sparse.csr_matrix(np.array(df_vector))\n",
    "# scipy.sparse.save_npz('resources/df_stackoverflow_5kanswers.npz', csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(scipy.sparse.load_npz('resources/df_stackoverflow_5kanswers.npz').todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = parse(\"nongit_resources/reviews_Electronics_5.json.gz\")\n",
    "# dataset = pd.DataFrame(list(dataset))\n",
    "# dataset = dataset.head(N_docs)\n",
    "# dataset.to_pickle(\"resources/reviews_Electronics_5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
